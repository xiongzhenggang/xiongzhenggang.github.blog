{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数\n",
    "我们回顾逻辑回归问题中我们的代价函数为：\n",
    "\n",
    "$ J\\left(\\theta \\right)=-\\frac{1}{m}\\left[\\sum_\\limits{i=1}^{m}{y}^{(i)}\\log{h_\\theta({x}^{(i)})}+\\left(1-{y}^{(i)}\\right)log\\left(1-h_\\theta\\left({x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2m}\\sum_\\limits{j=1}^{n}{\\theta_j}^{2} $\n",
    "\n",
    "在逻辑回归中，我们只有一个输出变量，又称标量（scalar），也只有一个因变量$y$，但是在神经网络中，我们可以有很多输出变量，我们的$h_\\theta(x)$是一个维度为$K$的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：$\\newcommand{\\subk}[1]{ #1_k }$ $$h_\\theta\\left(x\\right)\\in \\mathbb{R}^{K}$$ $${\\left({h_\\theta}\\left(x\\right)\\right)}_{i}={i}^{th} \\text{output}$$\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m} \\left[ \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{k} {y_k}^{(i)} \\log \\subk{(h_\\Theta(x^{(i)}))} + \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1- \\subk{\\left( h_\\Theta \\left( x^{(i)} \\right) \\right)} \\right) \\right] + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_{l+1}} \\left( \\Theta_{ji}^{(l)} \\right)^2$\n",
    "\n",
    "这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出$K​$个预测，基本上我们可以利用循环，对每一行特征都预测$K​$个不同结果，然后在利用循环在$K​$个预测中选择可能性最高的一个，将其与$y​$中的实际数据进行比较。\n",
    "\n",
    "正则化的那一项只是排除了每一层$\\theta_0$后，每一层的$\\theta$ 矩阵的和。最里层的循环$j$循环所有的行（由$s_{l+1}$ 层的激活单元数决定），循环$i$则循环所有的列，由该层（$s_l$层）的激活单元数所决定。即：$h_\\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播算法\n",
    "\n",
    "之前我们在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的$h_{\\theta}\\left(x\\right)$。\n",
    "\n",
    "现在，为了计算代价函数的偏导数$\\frac{\\partial}{\\partial\\Theta^{(l)}_{ij}}J\\left(\\Theta\\right)$，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从最后一层的误差开始计算，误差是激活单元的预测（${a^{(4)}}$）与实际值（$y^k$）之间的误差，（$k=1:k$）。 我们用$\\delta$来表示误差，则：$\\delta^{(4)}=a^{(4)}-y$ 我们利用这个误差值来计算前一层的误差：$\\delta^{(3)}=\\left({\\Theta^{(3)}}\\right)^{T}\\delta^{(4)}\\ast g'\\left(z^{(3)}\\right)$ 其中 $g'(z^{(3)})$是 $S$ 形函数的导数，$g'(z^{(3)})=a^{(3)}\\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\\delta^{(4)}$则是权重导致的误差的和。下一步是继续计算第二层的误差： $ \\delta^{(2)}=(\\Theta^{(2)})^{T}\\delta^{(3)}\\ast g'(z^{(2)})$ 因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有： $\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)=a_{j}^{(l)} \\delta_{i}^{l+1}$\n",
    "\n",
    "重要的是清楚地知道上面式子中上下标的含义：\n",
    "\n",
    "$l$ 代表目前所计算的是第几层。\n",
    "\n",
    "$j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。\n",
    "\n",
    "$i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。\n",
    "\n",
    "如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 $l$ 层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta^{(l)}{j}=\"error\" \\ of cost \\ for \\ a^{(l)}{j} \\ (unit \\ j \\ in \\ layer \\ l)$ 理解如下：\n",
    "\n",
    "$\\delta^{(l)}{j}$ 相当于是第 $l$ 层的第 $j$ 单元中得到的激活项的“误差”，即”正确“的 $a^{(l)}{j}$ 与计算得到的 $a^{(l)}_{j}$ 的差。\n",
    "\n",
    "而 $a^{(l)}{j}=g(z^{(l)})$ ，（g为sigmoid函数）。我们可以想象 $\\delta^{(l)}{j}$ 为函数求导时迈出的那一丁点微分，所以更准确的说 $\\delta^{(l)}{j}=\\frac{\\partial}{\\partial z^{(l)}{j}}cost(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度检验\n",
    "\n",
    "梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\\theta$，我们计算出在 $\\theta$-$\\varepsilon $ 处和 $\\theta$+$\\varepsilon $ 的代价值（$\\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\\theta$ 处的代价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何优化算法都需要一些初始的参数。\n",
    "\n",
    "到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。\n",
    "\n",
    "我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：\n",
    "\n",
    "Theta1 = rand(10, 11) * (2*eps) – eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结一下使用神经网络时的步骤：\n",
    "\n",
    "网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。\n",
    "\n",
    "第一层的单元数即我们训练集的特征数量。\n",
    "\n",
    "最后一层的单元数是我们训练集的结果的类的数量。\n",
    "\n",
    "如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。\n",
    "\n",
    "我们真正要决定的是隐藏层的层数和每个中间层的单元数。\n",
    "\n",
    "* 训练神经网络：\n",
    "\n",
    "参数的随机初始化\n",
    "\n",
    "利用正向传播方法计算所有的$h_{\\theta}(x)$\n",
    "\n",
    "编写计算代价函数 $J$ 的代码\n",
    "\n",
    "利用反向传播方法计算所有偏导数\n",
    "\n",
    "利用数值检验方法检验这些偏导数\n",
    "\n",
    "使用优化算法来最小化代价函数\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}